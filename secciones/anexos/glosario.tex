\section{Glosario de términos}

\subsection{Modelado y entrenamiento}
\textbf{Batch}: Conjunto de ejemplos utilizados en una sola iteración de entrenamiento. El uso de batches permite un entrenamiento más eficiente y estable.

\vspace{5mm}

\textbf{Early stopping}: Técnica de optimización que se utiliza durante el entrenamiento de modelos de aprendizaje profundo. Consiste en detener el entrenamiento cuando el modelo deja de mejorar en las métricas de evaluación durante un número determinado de iteraciones, con el fin de evitar el sobreajuste y ahorrar recursos computacionales.

\vspace{5mm}

\textbf{Entropía cruzada binaria}: Función de pérdida utilizada para medir la diferencia entre dos distribuciones de probabilidad en problemas de clasificación binaria. La entropía cruzada binaria penaliza las predicciones que están lejos de las verdaderas etiquetas, con el objetivo de que el modelo aprenda a minimizar esta diferencia.

\vspace{5mm}

\textbf{Época (Epoch)}: Pasada completa a través de todo el conjunto de datos durante el entrenamiento del modelo. Durante cada epoch, el modelo actualiza sus parámetros a medida que aprende a partir de los datos de entrenamiento. Generalmente, se requieren múltiples epochs para que un modelo converja a una solución óptima.

\vspace{5mm}

\textbf{Función de Pérdida}: Medida utilizada para evaluar la eficacia de un modelo durante el entrenamiento.

\vspace{5mm}

\textbf{Hiperparámetro}: Parámetro externo al modelo que se establece antes del proceso de entrenamiento. No son aprendidos por el modelo, pero afectan su rendimiento.

\vspace{5mm}

\textbf{Normalización}: Proceso de ajustar los valores de datos para que estén en un rango específico, generalmente entre 0 y 1, para mejorar la estabilidad y la velocidad de entrenamiento del modelo.

\vspace{5mm}

\textbf{Optimización}: Proceso de ajustar los parámetros de la red neuronal durante el entrenamiento para minimizar la función de pérdida.

\vspace{5mm}

\textbf{Tasa de Aprendizaje}: Hiperparámetro que controla el tamaño de los ajustes que realiza el modelo en sus pesos durante cada actualización. Un valor demasiado alto puede causar inestabilidad, mientras que un valor demasiado bajo puede hacer que el modelo aprenda muy lentamente o se estanque.

\vspace{5mm}


\subsection{Redes neuronales y arquitectura}
\textbf{Conditional Generative Adversarial Network}: Tipo de red neuronal generativa que, a partir de un conjunto de datos y una condición específica, genera nuevas muestras similares a las del conjunto de datos original, condicionadas por etiquetas.

\vspace{5mm}

\textbf{Colapso de modo}: Problema en el entrenamiento de una GAN en el que el generador aprende a producir un conjunto limitado de imágenes muy similares, perdiendo la diversidad en las salidas. Esto ocurre cuando el generador deja de explorar nuevas combinaciones y se enfoca en un solo tipo de resultado que engaña al discriminador.

\vspace{5mm}

\textbf{Dimensiones latentes}: Tamaño del vector de ruido que se usa como entrada al generador. Este vector suele tener una distribución aleatoria, y sus dimensiones determinan la complejidad de las imágenes que el generador puede producir.

\vspace{5mm}

\textbf{Discriminador}: Parte de una red generativa adversaria cuyo objetivo es distinguir entre datos reales y datos generados. Su función es mejorar la calidad de las imágenes generadas.

\vspace{5mm}

\textbf{Generador}: Parte de una red generativa adversaria que aprende a crear datos similares a los del conjunto de datos de entrenamiento. Su objetivo es engañar al discriminador.

\vspace{5mm}

\textbf{Neurona}: Unidad de procesamiento que recibe una o varias entradas, las procesa mediante una función matemática y produce una salida. Se organizan en capas y forman la estructura básica de las redes neuronales, permitiendo al modelo aprender y tomar decisiones.

\vspace{5mm}

\textbf{Red Neuronal}: Modelo computacional inspirado en la estructura del cerebro humano, compuesto por capas de neuronas interconectadas.

\vspace{5mm}

\textbf{Ruido aleatorio}: Entrada inicial al generador que sigue una distribución normalmente distribuida (generalmente una distribución normal). El generador toma este ruido y lo transforma en una imagen. Este proceso asegura que las imágenes generadas sean diversas, ya que pequeñas variaciones en el ruido conducen a la creación de imágenes diferentes.

\vspace{5mm}


\subsection{Datasets y datos}
\textbf{COCO (Common Objects in Context)}: Conjunto de datos que contiene imágenes con anotaciones detalladas. Incluye más de 330.000 imágenes con etiquetas y una resolución estándar de aproximadamente 640x480 píxeles.

\vspace{5mm}

\textbf{MNIST}: Conjunto de datos que contiene 70.000 imágenes de dígitos escritos a mano, divididas en un conjunto de entrenamiento y uno de prueba. Cada imagen es de 28x28 píxeles en escala de grises.

\vspace{5mm}

\textbf{Token}: Unidad básica de datos que representa una parte de un texto. Se utilizan para descomponer el texto en partes manejables, que pueden ser procesadas por modelos de lenguaje y redes neuronales.

\vspace{5mm}


\subsection{Interfaz y diseño}
\textbf{Socket}: Interfaz de software que permite la comunicación entre dos aplicaciones a través de una red. Define un punto final para la comunicación, especificando la dirección IP y el puerto en el que una aplicación escucha o envía datos.

\vspace{5mm}

\textbf{Wireframe}: Esquema visual básico de una interfaz de usuario, que muestra la disposición y estructura de los elementos clave sin entrar en detalles de diseño o contenido específico.

\vspace{5mm}

